{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten digit generation using GAN\n",
    "This is a simple implementation of GAN for generation of handwritten digit dataset using pytorch. For basic information about GANs implementation in PyTorch please refer to the link: https://www.youtube.com/watch?v=8l6NzV4Eryo\n",
    "\n",
    "### prerequisite\n",
    "- pytorch (0.4.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pytorch and torchvision \n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batch_size\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define helper functions \n",
    "- define to_var fuction to convert pytorch tensor to variable and for saving it in gpu. \n",
    "- define denorm for denormalizing the the output of the generator for saving it for the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "def denorm(x):\n",
    "    out = (x+1)/2\n",
    "    return out.clamp(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define transforms for transforming the image into tensor and normalizing it. Then define dataset and data_loader for training. We are using MNIST dataset which comes with torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))])\n",
    "mnist = datasets.MNIST(root='./data', train=True, \n",
    "                       transform=transform, download=True)\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the network architechture  for generator and discriminator\n",
    "- Generator: its a simple feed forward NN with increasing number of units and a Tanh activation funcition after the last layer\n",
    "- Discriminator: Similar to generator but the number of units are maximum in first layer and minimum in the layer. The activation function used for the last layer is Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Discriminator = nn.Sequential(\n",
    "                nn.Linear(784, 256),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(256, 1),\n",
    "                nn.Sigmoid())\n",
    "Generator = nn.Sequential(\n",
    "                nn.Linear(64, 256),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Linear(256, 784),\n",
    "                nn.Tanh())\n",
    "Discriminator = Discriminator.cuda()\n",
    "Generator = Generator.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define loss function and optimizers\n",
    "- Binary Cross entropy loss funciotn is used \n",
    "- Adam optimizer was used for both Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(Discriminator.parameters(), lr=0.0003)\n",
    "g_optimizer = torch.optim.Adam(Generator.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing training script \n",
    "- Number of epochs were taken as 80 \n",
    "- Other details about the training of GAN can be found in the video link given in the description at the begining of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/navidpanchi/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 0 | step: 299 | g_loss = 2.101189613342285 | d_loss = 0.8190131783485413 | D(x): 0.7217147946357727 | D(G(x)): 0.28404226899147034\n",
      " Epoch: 0 | step: 599 | g_loss = 1.971582293510437 | d_loss = 0.7654315233230591 | D(x): 0.7409430742263794 | D(G(x)): 0.25468164682388306\n",
      " Epoch: 1 | step: 299 | g_loss = 1.9672532081604004 | d_loss = 0.904847264289856 | D(x): 0.6897341012954712 | D(G(x)): 0.2614728808403015\n",
      " Epoch: 1 | step: 599 | g_loss = 1.831820011138916 | d_loss = 0.7765520811080933 | D(x): 0.7318035960197449 | D(G(x)): 0.2684221863746643\n",
      " Epoch: 2 | step: 299 | g_loss = 1.7677596807479858 | d_loss = 0.9350230097770691 | D(x): 0.7336815595626831 | D(G(x)): 0.33596086502075195\n",
      " Epoch: 2 | step: 599 | g_loss = 1.5004953145980835 | d_loss = 0.7788469195365906 | D(x): 0.7316011786460876 | D(G(x)): 0.23530524969100952\n",
      " Epoch: 3 | step: 299 | g_loss = 2.03519344329834 | d_loss = 0.6870499849319458 | D(x): 0.7712547183036804 | D(G(x)): 0.23793010413646698\n",
      " Epoch: 3 | step: 599 | g_loss = 2.7471282482147217 | d_loss = 0.9090229868888855 | D(x): 0.6933923959732056 | D(G(x)): 0.27115097641944885\n",
      " Epoch: 4 | step: 299 | g_loss = 1.586798071861267 | d_loss = 0.8103064894676208 | D(x): 0.753215491771698 | D(G(x)): 0.2737153172492981\n",
      " Epoch: 4 | step: 599 | g_loss = 2.097015619277954 | d_loss = 0.8279109001159668 | D(x): 0.734382152557373 | D(G(x)): 0.26166513562202454\n",
      " Epoch: 5 | step: 299 | g_loss = 1.5937305688858032 | d_loss = 0.7632128596305847 | D(x): 0.7663759589195251 | D(G(x)): 0.29707691073417664\n",
      " Epoch: 5 | step: 599 | g_loss = 2.104084014892578 | d_loss = 0.7464316487312317 | D(x): 0.7034680247306824 | D(G(x)): 0.20171672105789185\n",
      " Epoch: 6 | step: 299 | g_loss = 1.8593403100967407 | d_loss = 0.8041390180587769 | D(x): 0.7420321702957153 | D(G(x)): 0.2834239900112152\n",
      " Epoch: 6 | step: 599 | g_loss = 1.6979330778121948 | d_loss = 0.8621810674667358 | D(x): 0.7515363693237305 | D(G(x)): 0.30916735529899597\n",
      " Epoch: 7 | step: 299 | g_loss = 1.6452627182006836 | d_loss = 0.8443247079849243 | D(x): 0.7593876719474792 | D(G(x)): 0.326328843832016\n",
      " Epoch: 7 | step: 599 | g_loss = 1.4968938827514648 | d_loss = 0.8166463375091553 | D(x): 0.7862492203712463 | D(G(x)): 0.32159844040870667\n",
      " Epoch: 8 | step: 299 | g_loss = 1.7183418273925781 | d_loss = 0.8629337549209595 | D(x): 0.7633737921714783 | D(G(x)): 0.33248355984687805\n",
      " Epoch: 8 | step: 599 | g_loss = 1.8073307275772095 | d_loss = 0.8917075395584106 | D(x): 0.6953768134117126 | D(G(x)): 0.24717509746551514\n",
      " Epoch: 9 | step: 299 | g_loss = 1.5616450309753418 | d_loss = 0.9270403981208801 | D(x): 0.8160622715950012 | D(G(x)): 0.3668432831764221\n",
      " Epoch: 9 | step: 599 | g_loss = 1.7995781898498535 | d_loss = 0.7393144369125366 | D(x): 0.7529813051223755 | D(G(x)): 0.26965296268463135\n",
      " Epoch: 10 | step: 299 | g_loss = 1.6728073358535767 | d_loss = 0.7317147254943848 | D(x): 0.7206671833992004 | D(G(x)): 0.23394349217414856\n",
      " Epoch: 10 | step: 599 | g_loss = 1.509122371673584 | d_loss = 0.7995344400405884 | D(x): 0.7878657579421997 | D(G(x)): 0.32451292872428894\n",
      " Epoch: 11 | step: 299 | g_loss = 2.0433948040008545 | d_loss = 0.7657482624053955 | D(x): 0.8136657476425171 | D(G(x)): 0.3459380567073822\n",
      " Epoch: 11 | step: 599 | g_loss = 1.7129703760147095 | d_loss = 0.779250979423523 | D(x): 0.7878393530845642 | D(G(x)): 0.30891215801239014\n",
      " Epoch: 12 | step: 299 | g_loss = 1.9403833150863647 | d_loss = 0.845028817653656 | D(x): 0.7097105979919434 | D(G(x)): 0.28345051407814026\n",
      " Epoch: 12 | step: 599 | g_loss = 1.7375541925430298 | d_loss = 0.7912225127220154 | D(x): 0.764461100101471 | D(G(x)): 0.29060569405555725\n",
      " Epoch: 13 | step: 299 | g_loss = 1.8654948472976685 | d_loss = 0.6865870952606201 | D(x): 0.7392093539237976 | D(G(x)): 0.22127360105514526\n",
      " Epoch: 13 | step: 599 | g_loss = 1.8040640354156494 | d_loss = 0.6780414581298828 | D(x): 0.7170416116714478 | D(G(x)): 0.19739745557308197\n",
      " Epoch: 14 | step: 299 | g_loss = 2.315166711807251 | d_loss = 1.0616724491119385 | D(x): 0.5840816497802734 | D(G(x)): 0.20635823905467987\n",
      " Epoch: 14 | step: 599 | g_loss = 1.948330283164978 | d_loss = 0.898746907711029 | D(x): 0.6104625463485718 | D(G(x)): 0.184132382273674\n",
      " Epoch: 15 | step: 299 | g_loss = 2.0068721771240234 | d_loss = 0.7489942312240601 | D(x): 0.7355852127075195 | D(G(x)): 0.23437242209911346\n",
      " Epoch: 15 | step: 599 | g_loss = 2.288634777069092 | d_loss = 0.7786200046539307 | D(x): 0.7485812902450562 | D(G(x)): 0.25635382533073425\n",
      " Epoch: 16 | step: 299 | g_loss = 1.7192786931991577 | d_loss = 0.854405403137207 | D(x): 0.7974835634231567 | D(G(x)): 0.3417056202888489\n",
      " Epoch: 16 | step: 599 | g_loss = 1.6958264112472534 | d_loss = 0.8582332730293274 | D(x): 0.746752917766571 | D(G(x)): 0.31042009592056274\n",
      " Epoch: 17 | step: 299 | g_loss = 1.52970290184021 | d_loss = 0.9035607576370239 | D(x): 0.7335827946662903 | D(G(x)): 0.31033602356910706\n",
      " Epoch: 17 | step: 599 | g_loss = 1.6652426719665527 | d_loss = 0.9292110204696655 | D(x): 0.8051929473876953 | D(G(x)): 0.3821156322956085\n",
      " Epoch: 18 | step: 299 | g_loss = 1.8930199146270752 | d_loss = 0.8144322037696838 | D(x): 0.6730369329452515 | D(G(x)): 0.1897151917219162\n",
      " Epoch: 18 | step: 599 | g_loss = 1.893115520477295 | d_loss = 0.9567753076553345 | D(x): 0.7191279530525208 | D(G(x)): 0.3052985966205597\n",
      " Epoch: 19 | step: 299 | g_loss = 2.0759544372558594 | d_loss = 0.7971712350845337 | D(x): 0.7006548643112183 | D(G(x)): 0.2365425080060959\n",
      " Epoch: 19 | step: 599 | g_loss = 2.1906516551971436 | d_loss = 0.815904974937439 | D(x): 0.6846678256988525 | D(G(x)): 0.2157818228006363\n",
      " Epoch: 20 | step: 299 | g_loss = 1.4852616786956787 | d_loss = 0.818429172039032 | D(x): 0.7433390617370605 | D(G(x)): 0.29294392466545105\n",
      " Epoch: 20 | step: 599 | g_loss = 2.089893102645874 | d_loss = 0.8494757413864136 | D(x): 0.7273206114768982 | D(G(x)): 0.28605031967163086\n",
      " Epoch: 21 | step: 299 | g_loss = 1.7990303039550781 | d_loss = 0.7537993788719177 | D(x): 0.7664980292320251 | D(G(x)): 0.27517396211624146\n",
      " Epoch: 21 | step: 599 | g_loss = 1.9818724393844604 | d_loss = 0.8170260190963745 | D(x): 0.6795868277549744 | D(G(x)): 0.2101527750492096\n",
      " Epoch: 22 | step: 299 | g_loss = 1.7365753650665283 | d_loss = 0.7682750225067139 | D(x): 0.7552682161331177 | D(G(x)): 0.25608396530151367\n",
      " Epoch: 22 | step: 599 | g_loss = 1.8875024318695068 | d_loss = 0.7298229336738586 | D(x): 0.7517003417015076 | D(G(x)): 0.2648698389530182\n",
      " Epoch: 23 | step: 299 | g_loss = 1.959977388381958 | d_loss = 0.9048900008201599 | D(x): 0.7653617858886719 | D(G(x)): 0.3318478465080261\n",
      " Epoch: 23 | step: 599 | g_loss = 2.0624778270721436 | d_loss = 0.9172821044921875 | D(x): 0.7193595767021179 | D(G(x)): 0.2937336266040802\n",
      " Epoch: 24 | step: 299 | g_loss = 1.5834105014801025 | d_loss = 0.830869197845459 | D(x): 0.7024730443954468 | D(G(x)): 0.2760290801525116\n",
      " Epoch: 24 | step: 599 | g_loss = 1.6395856142044067 | d_loss = 0.8238478302955627 | D(x): 0.7232955098152161 | D(G(x)): 0.2387719303369522\n",
      " Epoch: 25 | step: 299 | g_loss = 1.5490351915359497 | d_loss = 0.8493369817733765 | D(x): 0.7596062421798706 | D(G(x)): 0.32159852981567383\n",
      " Epoch: 25 | step: 599 | g_loss = 2.0829410552978516 | d_loss = 0.7411142587661743 | D(x): 0.811716616153717 | D(G(x)): 0.278787761926651\n",
      " Epoch: 26 | step: 299 | g_loss = 1.8098734617233276 | d_loss = 0.7877721786499023 | D(x): 0.7122817635536194 | D(G(x)): 0.23194505274295807\n",
      " Epoch: 26 | step: 599 | g_loss = 2.012061357498169 | d_loss = 0.8931097984313965 | D(x): 0.6776880025863647 | D(G(x)): 0.2473307102918625\n",
      " Epoch: 27 | step: 299 | g_loss = 1.9291267395019531 | d_loss = 0.790729820728302 | D(x): 0.6588230133056641 | D(G(x)): 0.17471851408481598\n",
      " Epoch: 27 | step: 599 | g_loss = 1.971571683883667 | d_loss = 0.7363967299461365 | D(x): 0.7755200266838074 | D(G(x)): 0.2711031138896942\n",
      " Epoch: 28 | step: 299 | g_loss = 1.631485939025879 | d_loss = 0.8421429395675659 | D(x): 0.7170023322105408 | D(G(x)): 0.28883838653564453\n",
      " Epoch: 28 | step: 599 | g_loss = 2.4086358547210693 | d_loss = 0.7315064668655396 | D(x): 0.7620410919189453 | D(G(x)): 0.24893905222415924\n",
      " Epoch: 29 | step: 299 | g_loss = 1.8032037019729614 | d_loss = 0.7622465491294861 | D(x): 0.7552021741867065 | D(G(x)): 0.2606162428855896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 29 | step: 599 | g_loss = 1.4927273988723755 | d_loss = 0.7416038513183594 | D(x): 0.7581793665885925 | D(G(x)): 0.26405784487724304\n",
      " Epoch: 30 | step: 299 | g_loss = 2.048403263092041 | d_loss = 0.9189010262489319 | D(x): 0.7668018937110901 | D(G(x)): 0.3383224904537201\n",
      " Epoch: 30 | step: 599 | g_loss = 1.9411944150924683 | d_loss = 0.8158136606216431 | D(x): 0.7699710726737976 | D(G(x)): 0.27824297547340393\n",
      " Epoch: 31 | step: 299 | g_loss = 1.809350609779358 | d_loss = 0.9242625832557678 | D(x): 0.7158764600753784 | D(G(x)): 0.3154452443122864\n",
      " Epoch: 31 | step: 599 | g_loss = 1.6389538049697876 | d_loss = 0.8628527522087097 | D(x): 0.7144772410392761 | D(G(x)): 0.2748996615409851\n",
      " Epoch: 32 | step: 299 | g_loss = 1.8081386089324951 | d_loss = 0.7654416561126709 | D(x): 0.7053819894790649 | D(G(x)): 0.2237631380558014\n",
      " Epoch: 32 | step: 599 | g_loss = 1.9637451171875 | d_loss = 0.9808391332626343 | D(x): 0.6779628396034241 | D(G(x)): 0.2456500381231308\n",
      " Epoch: 33 | step: 299 | g_loss = 1.7009527683258057 | d_loss = 0.7318331003189087 | D(x): 0.7570311427116394 | D(G(x)): 0.2331569641828537\n",
      " Epoch: 33 | step: 599 | g_loss = 2.3039634227752686 | d_loss = 0.7487879991531372 | D(x): 0.6762226223945618 | D(G(x)): 0.16670076549053192\n",
      " Epoch: 34 | step: 299 | g_loss = 1.8269366025924683 | d_loss = 0.7707622051239014 | D(x): 0.735072135925293 | D(G(x)): 0.23929189145565033\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(80):\n",
    "    for step, batch in enumerate(data_loader, 0):\n",
    "        images, _ = batch\n",
    "        batch_size = images.size(0)\n",
    "        images = to_var(images.view(batch_size, -1))\n",
    "        \n",
    "        real_labels = to_var(torch.ones(batch_size))\n",
    "        fake_labels = to_var(torch.zeros(batch_size))\n",
    "        \n",
    "        #training discriminator\n",
    "        #real data\n",
    "        outputs = Discriminator(images)\n",
    "        d_real_loss = criterion(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "        \n",
    "        #fake data\n",
    "        z = to_var(torch.randn(batch_size, 64))\n",
    "        fake_images = Generator(z)\n",
    "        outputs = Discriminator(fake_images)\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "        \n",
    "        #backprop + optimization (discriminator)\n",
    "        d_loss = d_loss_fake + d_real_loss\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        #generator training\n",
    "        z = to_var(torch.randn(batch_size, 64))\n",
    "        fake_images = Generator(z)\n",
    "        outputs = Discriminator(fake_images)\n",
    "        \n",
    "        #maximize log(D(G(z))) \n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        \n",
    "        #backprop\n",
    "        g_optimizer.zero_grad()\n",
    "        d_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        if (step +1) % 300 == 0:\n",
    "            print(f' Epoch: {epoch} | step: {step} | g_loss = {g_loss} | d_loss = {d_loss} | D(x): {real_score.data.mean()} | D(G(x)): {fake_score.data.mean()}')\n",
    "        \n",
    "        if (epoch+1) == 1:\n",
    "            images = images.view(images.size(0), 1, 28, 28)\n",
    "            save_image(denorm(images.data), './data/real_images.png')\n",
    "            \n",
    "        fake_images = fake_images.view(fake_images.size(0),1,28,28)\n",
    "        save_image(denorm(fake_images.data), './data/fake_images-%d.png'%(epoch+1))\n",
    "\n",
    "#save the weights of generator and discriminator\n",
    "torch.save(Generator.state_dict(), './generator.pkl')\n",
    "torch.save(Discriminator.state_dict(), './discriminator.pkl')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
